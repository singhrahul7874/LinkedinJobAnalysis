{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f896ade8",
   "metadata": {},
   "source": [
    "#                                  Linkedin Scrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abf28d3",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0bd020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules and packages for Selenium web automation, Chrome web driver management, and data handling\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7784ed33",
   "metadata": {},
   "source": [
    "## Initializing our Automated Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03342c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Chrome web driver executable\n",
    "chrome_web = Service(r'D:\\UNIT_PROJECT_ML\\PYTHON\\SCRAPPING_CODE\\chromedriver.exe')\n",
    "\n",
    "# Initialize a Chrome WebDriver instance\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a24ed",
   "metadata": {},
   "source": [
    "## Creating Empty Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39384e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store job listing data\n",
    "data = {'Designation': [],\n",
    "        'Name': [],\n",
    "        'Location': [],\n",
    "        'Level_and_involvement': [],\n",
    "        'job_description': [],\n",
    "        'Total_applicants': [],\n",
    "        'Industry_and_Employee_count': [],\n",
    "        'LinkedIn_Followers': []}\n",
    "\n",
    "# The 'data' dictionary is used to store structured data related to job listings\n",
    "# Each key represents a specific attribute of a job listing, such as designation, name, location, etc.\n",
    "# The values associated with each key are initially empty lists, indicating that the data will be populated dynamically\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac177ed",
   "metadata": {},
   "source": [
    "## Maximizing our window and Hitting Linekdin Jobs page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1a25d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximize the browser window\n",
    "driver.maximize_window()\n",
    "\n",
    "# Open the LinkedIn website\n",
    "driver.get('https://www.linkedin.com/')\n",
    "\n",
    "# The 'driver.maximize_window()' method is used to maximize the browser window.\n",
    "# It ensures that the browser window occupies the entire screen, providing a better view and experience.\n",
    "\n",
    "# The 'driver.get()' method is used to navigate to the specified URL, in this case, 'https://www.linkedin.com/'.\n",
    "# It opens the LinkedIn website in the browser.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6637d1d3",
   "metadata": {},
   "source": [
    "## Signing in to our Linkedin account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72c6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mail = input('Enter your Email: ')\n",
    "password = input('Enter your Password: ')\n",
    "\n",
    "\n",
    "def Login_func():\n",
    "    \"\"\"\n",
    "    Perform login to the LinkedIn website.\n",
    "\n",
    "    This function enters the provided email and password into the respective fields on the login page,\n",
    "    clicks the login button, and waits for 5 seconds to allow for the login process to complete.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    Enter_mail = driver.find_element(By.XPATH, '/html/body/main/section[1]/div/div/form/div[2]/div[1]/input')\n",
    "    Enter_mail.send_keys(mail)\n",
    "    Enter_pass = driver.find_element(By.XPATH, '/html/body/main/section[1]/div/div/form/div[2]/div[2]/input')\n",
    "    Enter_pass.send_keys(password)\n",
    "    login_button = driver.find_element(By.XPATH, '/html/body/main/section[1]/div/div/form/button')\n",
    "    time.sleep(5)\n",
    "    login_button.click()\n",
    "\n",
    "\n",
    "# Call the Login_func() to perform the login operation\n",
    "Login_func()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d6ca15",
   "metadata": {},
   "source": [
    "## Scrapping the Features required for our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93845b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkdin_data_extraction(link):\n",
    "    \"\"\"\n",
    "    Extract data from a LinkedIn job listing page.\n",
    "\n",
    "    This function navigates to the provided LinkedIn job listing page, extracts various data points\n",
    "    such as job title, company name, location, job level, number of applicants, industry, job description,\n",
    "    and LinkedIn followers, and appends them to the respective lists in the 'data' dictionary.\n",
    "\n",
    "    Args:\n",
    "        link (str): The URL of the LinkedIn job listing page.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Open the provided link in the browser\n",
    "    driver.get(link)  \n",
    "    \n",
    "    # Find the HTML element (used for scrolling) on the page\n",
    "    bar = driver.find_element(By.XPATH, '/html')\n",
    "\n",
    "\n",
    "    for i in range(1, 21):\n",
    "        time.sleep(2.5)\n",
    "        try:\n",
    "            jobs = driver.find_element(By.XPATH, f'/html/body/div[5]/div[3]/div[4]/div/div/main/div/section['\n",
    "                                                  f'1]/div/ul/li[{i}]')\n",
    "            time.sleep(1)\n",
    "            jobs.click()\n",
    "        except:\n",
    "            print(link[link.find('start'):])\n",
    "            return None\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            job_title = driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div['\n",
    "                                                       '4]/div/div/main/div/section[2]/div/div[2]/div['\n",
    "                                                       '1]/div/div[1]/div/div[1]/div[1]/a/h2')\n",
    "            \n",
    "            # Append job title to 'Designation' list in 'data' dictionary\n",
    "            data['Designation'].append(job_title.text)  \n",
    "\n",
    "        except NoSuchElementException:\n",
    "            data['Designation'].append(np.nan)\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            company_name = driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div['\n",
    "                                                          '4]/div/div/main/div/section[2]/div/div[2]/div['\n",
    "                                                          '1]/div/div[1]/div/div[1]/div[1]/div[1]/span['\n",
    "                                                          '1]/span[1]/a')\n",
    "            \n",
    "            # Append company name to 'Name' list in 'data' dictionary\n",
    "            data['Name'].append(company_name.text)  \n",
    "\n",
    "        except NoSuchElementException:\n",
    "            data['Name'].append(np.nan)\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            com_location = driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div['\n",
    "                                                          '4]/div/div/main/div/section[2]/div/div[2]/div['\n",
    "                                                          '1]/div/div[1]/div/div[1]/div[1]/div[1]/span['\n",
    "                                                          '1]/span[2]')\n",
    "            \n",
    "            # Append location to 'Location' list in 'data' dictionary\n",
    "            data['Location'].append(com_location.text)  \n",
    "\n",
    "        except NoSuchElementException:\n",
    "            data['Location'].append(np.nan)\n",
    "        time.sleep(1)\n",
    "\n",
    "        for x in range(4):\n",
    "            # Scroll down the page\n",
    "            bar.send_keys(Keys.ARROW_DOWN)  \n",
    "\n",
    "        try:\n",
    "            job_level_and_type = driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div['\n",
    "                                                                '4]/div/div/main/div/section[2]/div/div['\n",
    "                                                                '2]/div[1]/div/div[1]/div/div[1]/div[1]/div['\n",
    "                                                                '2]/ul/li[1]/span')\n",
    "            \n",
    "            # Append job level to 'Level_and_involvement' list in 'data' dictionary\n",
    "            data['Level_and_involvement'].append(job_level_and_type.text)  \n",
    "\n",
    "        except NoSuchElementException:\n",
    "            data['Level_and_involvement'].append(np.nan)\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            num_of_applicants = driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div['\n",
    "                                                               '4]/div/div/main/div/section[2]/div/div['\n",
    "                                                               '2]/div[1]/div/div[1]/div/div[1]/div[1]/div['\n",
    "                                                               '1]/span[2]/span[2]/span')\n",
    "            \n",
    "            # Append number of applicants to 'Total_applicants' list in 'data' dictionary\n",
    "            data['Total_applicants'].append(num_of_applicants.text)  \n",
    "\n",
    "        except NoSuchElementException:\n",
    "            data['Total_applicants'].append(np.nan)\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            com_industry_and_employee_num = driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div['\n",
    "                                                                           '4]/div/div/main/div/section['\n",
    "                                                                           '2]/div/div[2]/div[1]/div/div['\n",
    "                                                                           '1]/div/div[1]/div[1]/div['\n",
    "                                                                           '2]/ul/li[2]/span')\n",
    "            \n",
    "            # Append industry and employee count to 'Industry_and_Employee_count' list in 'data' dictionary\n",
    "            data['Industry_and_Employee_count'].append(com_industry_and_employee_num.text)  \n",
    "\n",
    "        except NoSuchElementException:\n",
    "            data['Industry_and_Employee_count'].append(np.nan)\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            job_description = driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div['\n",
    "                                                             '4]/div/div/main/div/section[2]/div/div[2]/div['\n",
    "                                                             '1]/div/div[4]/article')\n",
    "            \n",
    "            # Append job description to 'job_description' list in 'data' dictionary\n",
    "            data['job_description'].append(job_description.text)  \n",
    "\n",
    "        except NoSuchElementException:\n",
    "            data['job_description'].append(np.nan)\n",
    "        time.sleep(1)\n",
    "\n",
    "        sec_bar = driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div[4]/div/div/main/div/section['\n",
    "                                                 '2]/div')\n",
    "        # Scroll down the secondary bar\n",
    "        driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', sec_bar)  \n",
    "        time.sleep(1.5)\n",
    "\n",
    "        try:\n",
    "            followers = driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div['\n",
    "                                                       '4]/div/div/main/div/section[2]/div/div[2]/div['\n",
    "                                                       '1]/div/section/section/div[1]/div[1]/div/div[2]/div['\n",
    "                                                       '2]')\n",
    "            # Append LinkedIn followers count to 'LinkedIn_Followers' list in 'data' dictionary\n",
    "            data['LinkedIn_Followers'].append(followers.text)  \n",
    "        except NoSuchElementException:\n",
    "            data['LinkedIn_Followers'].append(np.nan)\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222895ad",
   "metadata": {},
   "source": [
    "## Getting Link of different pages for extracting data-points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc980260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_linkedin_jobs():\n",
    "    \"\"\"\n",
    "    Scrapes job data from multiple pages on LinkedIn job search.\n",
    "\n",
    "    The function navigates through different pages of job search results on LinkedIn,\n",
    "    extracts relevant job details, and stores them in the 'data' dictionary.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Create a list of values from 0 to 400 (inclusive) in increments of 25\n",
    "    flag = [i for i in range(0, 401, 25)]  \n",
    "    \n",
    "    # Remove the value 0 from the list\n",
    "    flag.remove(0)  \n",
    "    \n",
    "    # Insert the value 1 at the beginning of the list\n",
    "    flag.insert(0, 1)  \n",
    "\n",
    "    for i in flag:\n",
    "        # Calling linkdin_data_extraction func and passing link to different pages as arugment\n",
    "        linkdin_data_extraction(f'https://www.linkedin.com/jobs/search/?currentJobId=3365364752&f_C=165158%2C1353%2C58396'\n",
    "                                f'%2C51692521%2C1283%2C6567943%2C1073%2C18145101%2C12770%2C9215331%2C4300%2C1318%2C3178'\n",
    "                                f'%2C86813252%2C6339%2C210064%2C14439560&f_E=1%2C2%2C3%2C4%2C5%2C6&geoId=102713980&location'\n",
    "                                f'=India&refresh=true&start={i}')\n",
    "\n",
    "# Call the function to start scraping\n",
    "scrape_linkedin_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5cc5ea",
   "metadata": {},
   "source": [
    "##  Creates a CSV file of the collected data from scraping the LinkedIn website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DataFrame():\n",
    "    \"\"\"\n",
    "    Creates a DataFrame from the collected job data and saves it as a CSV file.\n",
    "\n",
    "    The function converts the 'data' dictionary into a DataFrame using pandas,\n",
    "    and then saves the DataFrame as a CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a DataFrame from the 'data' dictionary\n",
    "    df = pd.DataFrame(data)  \n",
    "    \n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(r\"..\\PYTHON\\EXCEL\\SCRAPED_DATA\\rahul_scrapped data.csv\", index=False)\n",
    "\n",
    "# Call the func to create DataFrame\n",
    "create_DataFrame()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
