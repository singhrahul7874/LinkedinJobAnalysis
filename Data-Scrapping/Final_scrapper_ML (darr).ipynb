{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f896ade8",
   "metadata": {},
   "source": [
    "#                                  Linkdin Scrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abf28d3",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0bd020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7784ed33",
   "metadata": {},
   "source": [
    "## Initializing our Automated Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03342c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_web = Service(r'D:\\UNIT_PROJECT_ML\\PYTHON\\SCRAPPING_CODE\\chromedriver.exe')\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a24ed",
   "metadata": {},
   "source": [
    "## Creating Empty Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39384e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Designation': [],\n",
    "        'Name': [],\n",
    "        'Location': [],\n",
    "        'Level_and_involvement': [],\n",
    "        'job_description': [],\n",
    "        'Total_applicants': [],\n",
    "        'Industry_and_Employee_count': [],\n",
    "        'LinkedIn_Followers': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac177ed",
   "metadata": {},
   "source": [
    "## Maximizing our window and Hitting Linkdin Jobs page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1a25d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.maximize_window()\n",
    "driver.get('https://www.linkedin.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6637d1d3",
   "metadata": {},
   "source": [
    "## Signing in to our Linkdin account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72c6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mail = input('Enter your Email')\n",
    "password = input('Enter your Password')\n",
    "\n",
    "\n",
    "def Login_func():\n",
    "    Enter_mail = driver.find_element(By.XPATH, '/html/body/main/section[1]/div/div/form/div[2]/div[1]/input')\n",
    "    Enter_mail.send_keys(mail)\n",
    "    Enter_pass = driver.find_element(By.XPATH, '/html/body/main/section[1]/div/div/form/div[2]/div[2]/input')\n",
    "    Enter_pass.send_keys(password)\n",
    "    login_button = self.driver.find_element(By.XPATH, '/html/body/main/section[1]/div/div/form/button')\n",
    "    time.sleep(5)\n",
    "    login_button.click()\n",
    "\n",
    "Login_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d6ca15",
   "metadata": {},
   "source": [
    "## Scrapping the Features required for our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93845b13",
   "metadata": {},
   "outputs": [],
   "source": [
    " def linkdin_data_extraction(link):\n",
    "        driver.get(link)\n",
    "\n",
    "        bar = driver.find_element(By.XPATH, '/html')\n",
    "\n",
    "        for i in range(1, 21):\n",
    "            time.sleep(2.5)\n",
    "            try:\n",
    "                jobs = driver.find_element(By.XPATH, f'/html/body/div[5]/div[3]/div[4]/div/div/main/div/section['\n",
    "                                                          f'1]/div/ul/li[{i}]')\n",
    "                time.sleep(1)\n",
    "                jobs.click()\n",
    "            except:\n",
    "                print(link[link.find('start'):])\n",
    "                return None\n",
    "            time.sleep(1)\n",
    "\n",
    "            try:\n",
    "                job_title = driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div['\n",
    "                                                               '4]/div/div/main/div/section[2]/div/div[2]/div['\n",
    "                                                               '1]/div/div[1]/div/div[1]/div[1]/a/h2')\n",
    "                data['Designation'].append(job_title.text)\n",
    "                \n",
    "            except NoSuchElementException:\n",
    "                data['Designation'].append(np.nan)\n",
    "            time.sleep(1)\n",
    "\n",
    "            try:\n",
    "                company_name = driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div['\n",
    "                                                                  '4]/div/div/main/div/section[2]/div/div[2]/div['\n",
    "                                                                  '1]/div/div[1]/div/div[1]/div[1]/div[1]/span['\n",
    "                                                                  '1]/span[1]/a')\n",
    "                data['Name'].append(company_name.text)\n",
    "                \n",
    "            except NoSuchElementException:\n",
    "                data['Name'].append(np.nan)\n",
    "            time.sleep(1)\n",
    "\n",
    "            try:\n",
    "                com_location = driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div['\n",
    "                                                                  '4]/div/div/main/div/section[2]/div/div[2]/div['\n",
    "                                                                  '1]/div/div[1]/div/div[1]/div[1]/div[1]/span['\n",
    "                                                                  '1]/span[2]')\n",
    "                data['Location'].append(com_location.text)\n",
    "                \n",
    "            except NoSuchElementException:\n",
    "                data['Location'].append(np.nan)\n",
    "            time.sleep(1)\n",
    "\n",
    "            for x in range(4):\n",
    "                bar.send_keys(Keys.ARROW_DOWN)\n",
    "\n",
    "            try:\n",
    "                job_level_and_type = driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div['\n",
    "                                                                        '4]/div/div/main/div/section[2]/div/div['\n",
    "                                                                        '2]/div[1]/div/div[1]/div/div[1]/div[1]/div['\n",
    "                                                                        '2]/ul/li[1]/span')\n",
    "                data['Level_and_involvement'].append(job_level_and_type.text)\n",
    "                \n",
    "            except NoSuchElementException:\n",
    "                data['Level_and_involvement'].append(np.nan)\n",
    "            time.sleep(1)\n",
    "\n",
    "            try:\n",
    "                num_of_applicants = driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div['\n",
    "                                                                       '4]/div/div/main/div/section[2]/div/div['\n",
    "                                                                       '2]/div[1]/div/div[1]/div/div[1]/div[1]/div['\n",
    "                                                                       '1]/span[2]/span[2]/span')\n",
    "                data['Total_applicants'].append(num_of_applicants.text)\n",
    "                \n",
    "            except NoSuchElementException:\n",
    "                data['Total_applicants'].append(np.nan)\n",
    "            time.sleep(1)\n",
    "\n",
    "            try:\n",
    "                com_industry_and_employee_num = driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div['\n",
    "                                                                                   '4]/div/div/main/div/section['\n",
    "                                                                                   '2]/div/div[2]/div[1]/div/div['\n",
    "                                                                                   '1]/div/div[1]/div[1]/div['\n",
    "                                                                                   '2]/ul/li[2]/span')\n",
    "                data['Industry_and_Employee_count'].append(com_industry_and_employee_num.text)\n",
    "                \n",
    "            except NoSuchElementException:\n",
    "                data['Industry_and_Employee_count'].append(np.nan)\n",
    "            time.sleep(1)\n",
    "\n",
    "            try:\n",
    "                job_description = driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div['\n",
    "                                                                     '4]/div/div/main/div/section[2]/div/div[2]/div['\n",
    "                                                                     '1]/div/div[4]/article')\n",
    "\n",
    "                data['job_description'].append(job_description.text)\n",
    "                \n",
    "            except NoSuchElementException:\n",
    "                data['job_description'].append(np.nan)\n",
    "            time.sleep(1)\n",
    "\n",
    "            sec_bar = driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div[4]/div/div/main/div/section['\n",
    "                                                         '2]/div')\n",
    "            driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', sec_bar)\n",
    "            time.sleep(1.5)\n",
    "\n",
    "            try:\n",
    "                followers = driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div['\n",
    "                                                               '4]/div/div/main/div/section[2]/div/div[2]/div['\n",
    "                                                               '1]/div/section/section/div[1]/div[1]/div/div[2]/div['\n",
    "                                                               '2]')\n",
    "                data['LinkedIn_Followers'].append(followers.text)\n",
    "            except NoSuchElementException:\n",
    "                data['LinkedIn_Followers'].append(np.nan)\n",
    "            time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222895ad",
   "metadata": {},
   "source": [
    "## Getting Link of different pages for extracting data-points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc980260",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = [i for i in range(0, 401, 25)]\n",
    "flag.remove(0)\n",
    "flag.insert(0, 1)\n",
    "\n",
    "for i in flag:\n",
    "    linkdin_data_extraction(f'https://www.linkedin.com/jobs/search/?currentJobId=3365364752&f_C=165158%2C1353%2C58396'\n",
    "                        f'%2C51692521%2C1283%2C6567943%2C1073%2C18145101%2C12770%2C9215331%2C4300%2C1318%2C3178'\n",
    "                        f'%2C86813252%2C6339%2C210064%2C14439560&f_E=1%2C2%2C3%2C4%2C5%2C6&geoId=102713980&location'\n",
    "                        f'=India&refresh=true&start={i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5cc5ea",
   "metadata": {},
   "source": [
    "## Creating a CSV file of the data collected in Scrapping linkedin website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DataFrame():\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(r\"..\\PYTHON\\EXCEL\\SCRAPED_DATA\\rahul_scrapped data.csv\", index=False)\n",
    "    \n",
    "    \n",
    "create_DataFrame()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
